
Tags: #ml #eda 

------------------------------------------
Feature selection and feature engineering are both important techniques in machine learning for improving the performance of models, but they refer to different aspects of the process.

Feature selection is the process of selecting a subset of the available features (i.e., variables) that are most relevant to the prediction task. This is typically done to reduce the dimensionality of the data and improve model performance, as using all available features can lead to overfitting and increased computational complexity. Feature selection techniques can be either supervised or unsupervised, and they can be based on statistical tests, machine learning algorithms, or expert knowledge. Examples of feature selection techniques include filtering methods (e.g., correlation-based feature selection), wrapper methods (e.g., recursive feature elimination), and embedded methods (e.g., Lasso regression).

[[Feature Engineering]] , on the other hand, is the process of creating new features or transforming existing features to improve the performance of machine learning models. This involves using domain knowledge or creativity to extract information from the data that may not be captured by the original features. Feature engineering techniques can include scaling or normalizing features, creating new features through mathematical operations (e.g., ratios, logarithms), combining features through feature crosses or interactions, and encoding categorical variables using techniques like one-hot encoding or ordinal encoding.

Overall, the main difference between feature selection and feature engineering is that feature selection is focused on reducing the number of features used in a model, while feature engineering is focused on improving the quality of the features used in a model. Both techniques can be used in combination to improve the performance of [[Machine Learning]] models.




---------------------
#### links:
[[]]
[[]]